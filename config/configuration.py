"""
Configura√ß√µes globais da aplica√ß√£o NFe Processor Agent.

Este m√≥dulo carrega todas as configura√ß√µes a partir de vari√°veis de ambiente
e define valores padr√£o para a aplica√ß√£o.
"""

import os
from enum import Enum
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv

# =====================================================
# Carregar vari√°veis de ambiente do arquivo .env
# =====================================================

# Encontrar arquivo .env na raiz do projeto
ENV_FILE = Path(__file__).parent.parent / ".env"
if ENV_FILE.exists():
    load_dotenv(ENV_FILE)
else:
    load_dotenv()


# =====================================================
# ENUMS - Definir tipos e provedores suportados
# =====================================================

class LLMProvider(str, Enum):
    """Provedores de LLM suportados."""
    OPENAI = "openai"
    GROQ = "groq"
    GEMINI = "gemini"
    CLAUDE = "claude"
    OLLAMA = "ollama"  # Para modelos locais
    
    def __str__(self) -> str:
        """Representa√ß√£o em string do provider."""
        names = {
            "openai": "OpenAI",
            "groq": "Groq",
            "gemini": "Google Gemini",
            "claude": "Claude (Anthropic)",
            "ollama": "Ollama (Local)",
        }
        return names.get(self.value, self.value)


class LLMModel(str, Enum):
    """Modelos de LLM suportados por provider."""
    # OpenAI
    GPT_4O = "gpt-4o"
    GPT_4O_MINI = "gpt-4o-mini"
    GPT_4_TURBO = "gpt-4-turbo"
    
    # Groq
    LLAMA_3 = "llama-3.3-70b-versatile"
    LLAMA_2 = "llama2-70b-4096"
    
    # Google Gemini
    GEMINI_PRO = "gemini-pro"
    GEMINI_1_5_PRO = "gemini-1.5-pro"
    
    # Anthropic Claude
    CLAUDE_3_OPUS = "claude-3-opus-20240229"
    CLAUDE_3_SONNET = "claude-3-sonnet-20240229"
    CLAUDE_3_HAIKU = "claude-3-haiku-20240307"
    
    # Ollama (local)
    OLLAMA_LLAMA2 = "llama2"
    OLLAMA_MISTRAL = "mistral"


# Mapa de modelos por provider
MODELS_BY_PROVIDER = {
    LLMProvider.OPENAI: [
        LLMModel.GPT_4O,
        LLMModel.GPT_4O_MINI,
        LLMModel.GPT_4_TURBO,
    ],
    LLMProvider.GROQ: [
        LLMModel.LLAMA_3,
        LLMModel.LLAMA_2,
    ],
    LLMProvider.GEMINI: [
        LLMModel.GEMINI_PRO,
        LLMModel.GEMINI_1_5_PRO,
    ],
    LLMProvider.CLAUDE: [
        LLMModel.CLAUDE_3_OPUS,
        LLMModel.CLAUDE_3_SONNET,
        LLMModel.CLAUDE_3_HAIKU,
    ],
    LLMProvider.OLLAMA: [
        LLMModel.OLLAMA_LLAMA2,
        LLMModel.OLLAMA_MISTRAL,
    ],
}


# =====================================================
# Caminhos do Projeto
# =====================================================

# Diret√≥rio raiz do projeto
PROJECT_ROOT = Path(__file__).parent.parent.absolute()

# Diret√≥rio source
SRC_DIR = PROJECT_ROOT / "src"

# Diret√≥rio de dados
DATA_DIR = PROJECT_ROOT / "data"
DATA_SAMPLES_DIR = DATA_DIR / "samples"
DATA_PROCESSED_DIR = DATA_DIR / "processed"

# Diret√≥rio de logs
LOGS_DIR = PROJECT_ROOT / "logs"

# Diret√≥rio de configura√ß√£o
CONFIG_DIR = PROJECT_ROOT / "config"

# Criar diret√≥rios se n√£o existirem
for directory in [DATA_DIR, DATA_SAMPLES_DIR, DATA_PROCESSED_DIR, LOGS_DIR]:
    directory.mkdir(parents=True, exist_ok=True)


# =====================================================
# Configura√ß√µes de Banco de Dados
# =====================================================

# Caminho do banco de dados SQLite
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    f"sqlite:///{DATA_PROCESSED_DIR}/notas_fiscais.db"
)

# Usar path direto para SQLite (sem prefixo sqlite:///)
DATABASE_PATH = DATA_PROCESSED_DIR / "notas_fiscais.db"

# Timeout para conex√µes com banco de dados (em segundos)
DATABASE_TIMEOUT = int(os.getenv("DATABASE_TIMEOUT", "30"))

# Echo SQL para debug
DATABASE_ECHO = os.getenv("DATABASE_ECHO", "False").lower() == "true"


# =====================================================
# Configura√ß√µes de Logging
# =====================================================

# N√≠vel de logging
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# Formato de log
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Arquivo de log principal
LOG_FILE = LOGS_DIR / "app.log"

# Arquivo de log do parser
LOG_FILE_PARSER = LOGS_DIR / "parser.log"

# Arquivo de log do agente
LOG_FILE_AGENT = LOGS_DIR / "agent.log"

# Tamanho m√°ximo de arquivo de log (em MB)
LOG_MAX_SIZE_MB = int(os.getenv("LOG_MAX_SIZE_MB", "10"))

# N√∫mero m√°ximo de arquivos de backup
LOG_BACKUP_COUNT = int(os.getenv("LOG_BACKUP_COUNT", "5"))


# =====================================================
# Configura√ß√µes de LLM (M√∫ltiplos Provedores)
# =====================================================

# Provedor padr√£o (pode ser alterado via Streamlit)
DEFAULT_LLM_PROVIDER = LLMProvider.GROQ  # Groq √© gratuito e r√°pido!

# Modelo padr√£o por provider (ser√° selecionado na interface)
DEFAULT_MODELS = {
    LLMProvider.OPENAI: LLMModel.GPT_4O_MINI,
    LLMProvider.GROQ: LLMModel.LLAMA_3,
    LLMProvider.GEMINI: LLMModel.GEMINI_PRO,
    LLMProvider.CLAUDE: LLMModel.CLAUDE_3_HAIKU,
    LLMProvider.OLLAMA: LLMModel.OLLAMA_LLAMA2,
}

# Temperatura (criatividade do modelo) - pode variar por provider
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))

# Token limit (pode variar por modelo)
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2000"))

# Timeout para requisi√ß√µes LLM (em segundos)
LLM_TIMEOUT = int(os.getenv("LLM_TIMEOUT", "60"))

# URL base para Ollama (se usando modelos locais)
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


# =====================================================
# Configura√ß√µes de Upload de Arquivos
# =====================================================

# Tamanho m√°ximo de upload (em MB)
MAX_UPLOAD_SIZE_MB = int(os.getenv("MAX_UPLOAD_SIZE_MB", "100"))

# Formatos aceitos
ALLOWED_FILE_EXTENSIONS = [".xml", ".pdf", ".zip"]

# Diret√≥rio tempor√°rio para uploads
TEMP_DIR = DATA_DIR / "temp"
TEMP_DIR.mkdir(parents=True, exist_ok=True)


# =====================================================
# Configura√ß√µes de Processamento
# =====================================================

# N√∫mero m√°ximo de notas para processar em batch
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "10"))

# Timeout para processamento de uma nota (em segundos)
PROCESSING_TIMEOUT = int(os.getenv("PROCESSING_TIMEOUT", "300"))

# Habilitar valida√ß√£o rigorosa
STRICT_VALIDATION = os.getenv("STRICT_VALIDATION", "True").lower() == "true"


# =====================================================
# Configura√ß√µes de Streamlit
# =====================================================

# T√≠tulo da aplica√ß√£o
STREAMLIT_TITLE = "NFe Processor Agent üìÑ"

# √çcone da aplica√ß√£o (emoji)
STREAMLIT_ICON = "üìÑ"

# Layout da p√°gina
STREAMLIT_LAYOUT = "wide"

# Tema
STREAMLIT_THEME = os.getenv("STREAMLIT_THEME", "light")


# =====================================================
# Configura√ß√µes de Mem√≥ria do Agente
# =====================================================

# Tamanho m√°ximo do hist√≥rico de mem√≥ria
MEMORY_MAX_HISTORY = int(os.getenv("MEMORY_MAX_HISTORY", "50"))

# Habilitar persist√™ncia de mem√≥ria
MEMORY_PERSIST = os.getenv("MEMORY_PERSIST", "True").lower() == "true"

# Arquivo para salvar mem√≥ria
MEMORY_FILE = DATA_PROCESSED_DIR / "agent_memory.json"


# =====================================================
# Configura√ß√µes de Relat√≥rios
# =====================================================

# Formato padr√£o de exporta√ß√£o
REPORT_DEFAULT_FORMAT = os.getenv("REPORT_DEFAULT_FORMAT", "html")

# Habilitar gr√°ficos interativos (Plotly)
REPORT_INTERACTIVE_CHARTS = os.getenv("REPORT_INTERACTIVE_CHARTS", "True").lower() == "true"

# Paleta de cores para gr√°ficos
REPORT_COLOR_PALETTE = "Set2"


# =====================================================
# Configura√ß√µes de Desenvolvimento
# =====================================================

# Modo debug
DEBUG = os.getenv("DEBUG", "False").lower() == "true"

# Modo teste
TESTING = os.getenv("TESTING", "False").lower() == "true"

# Dados de exemplo para teste
USE_SAMPLE_DATA = os.getenv("USE_SAMPLE_DATA", "False").lower() == "true"


# =====================================================
# Fun√ß√£o auxiliar para obter modelos por provider
# =====================================================

def get_models_for_provider(provider: LLMProvider) -> list[LLMModel]:
    """
    Retorna lista de modelos dispon√≠veis para um provider espec√≠fico.
    
    Args:
        provider: O provedor de LLM
        
    Returns:
        Lista de modelos dispon√≠veis para o provider
    """
    return MODELS_BY_PROVIDER.get(provider, [])


def get_provider_display_name(provider: LLMProvider) -> str:
    """
    Retorna nome de exibi√ß√£o para um provider.
    
    Args:
        provider: O provedor de LLM
        
    Returns:
        String de exibi√ß√£o do provider
    """
    return str(provider)


# =====================================================
# Fun√ß√£o auxiliar para validar config
# =====================================================

def validate_config() -> dict:
    """
    Valida as configura√ß√µes e retorna um dicion√°rio com status.
    
    Returns:
        dict: Dicion√°rio com keys "valid" (bool) e "issues" (list)
    """
    issues: list[str] = []
    
    # Validar diret√≥rios cr√≠ticos
    if not SRC_DIR.exists():
        issues.append(f"Diret√≥rio src n√£o encontrado: {SRC_DIR}")
    
    if not DATA_DIR.exists():
        issues.append(f"Diret√≥rio data n√£o criado: {DATA_DIR}")
    
    # Validar limites
    if MAX_UPLOAD_SIZE_MB <= 0:
        issues.append("MAX_UPLOAD_SIZE_MB deve ser positivo")
    
    if BATCH_SIZE <= 0:
        issues.append("BATCH_SIZE deve ser positivo")
    
    # Validar LLM Provider padr√£o
    if DEFAULT_LLM_PROVIDER not in LLMProvider:
        issues.append(f"Provider padr√£o inv√°lido: {DEFAULT_LLM_PROVIDER}")
    
    return {
        "valid": len(issues) == 0,
        "issues": issues
    }
